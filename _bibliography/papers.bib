---
---

@INPROCEEDINGS{8923937,
  author={Franco, Gabriel and Henrique Fonseca Ribeiro, Marcos and Comarela, Giovanni},
  booktitle={2019 8th Brazilian Conference on Intelligent Systems (BRACIS)}, 
  title={Towards an Interpretable Metric for DOTA 2 Players: An Unsupervised Learning Approach}, 
  year={2019},
  volume={},
  number={},
  pages={341-346},
  doi={10.1109/BRACIS.2019.00067}
}

@article{gomes2016plasmodium,
  title={Plasmodium Falciparum Infection: In Silico Preliminary Studies},
  author={Gomes, Andr{\'e}ia Patricia and Moreira, Brenda Silveira Valles and Dias, Felipe Jos{\'e} Dutra and Inoue, Victor Hiroshi Bastos and Franco, Gabriel Vita Silva and de Souza Gomes, Daniela and de Paiva Oliveira, Alcione and Cerqueira, Fabio Ribeiro and Miguel, Paulo S{\'e}rgio Balbino and Santana, Luiz Alberto and others},
  journal={Abak{\'o}s},
  volume={5},
  number={1},
  pages={63--83},
  year={2016}
}

@inproceedings{10.1145/3580305.3599307,
author = {Franco, Gabriel and Crovella, Mark and Comarela, Giovanni},
title = {Dependence and Model Selection in LLP: The Problem of Variants},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599307},
doi = {10.1145/3580305.3599307},
abstract = {The problem of Learning from Label Proportions (LLP) has received considerable research attention and has numerous practical applications. In LLP, a hypothesis assigning labels to items is learned using knowledge of only the proportion of labels found in predefined groups, called bags. While a number of algorithmic approaches to learning in this context have been proposed, very little work has addressed the model selection problem for LLP. Nonetheless, it is not obvious how to extend straightforward model selection approaches to LLP, in part because of the lack of item labels. More fundamentally, we argue that a careful approach to model selection for LLP requires consideration of the dependence structure that exists between bags, items, and labels. In this paper we formalize this structure and show how it affects model selection. We show how this leads to improved methods of model selection that we demonstrate outperform the state of the art over a wide range of datasets and LLP algorithms.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {470â€“481},
numpages = {12},
keywords = {weakly supervised learning, learning from label proportions, hyperparameter selection},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{franco2023evaluating,
  title={Evaluating LLP Methods: Challenges and Approaches},
  author={Franco, Gabriel and Comarela, Giovanni and Crovella, Mark},
  journal={arXiv preprint arXiv:2310.19065},
  year={2023}
}

@article{franco2024sparse,
  title={Sparse Attention Decomposition Applied to Circuit Tracing},
  author={Franco, Gabriel and Crovella, Mark},
  journal={arXiv preprint arXiv:2410.00340},
  year={2024}
}

@inproceedings{calais-etal-2025-disentangling,
    title = "Disentangling Text and Math in Word Problems: Evidence for the Bidimensional Structure of Large Language Models' Reasoning",
    author = "Calais, Pedro  and
      Franco, Gabriel  and
      Tang, Zilu  and
      Nikas, Themistoklis  and
      Jr., Wagner Meira  and
      Terzi, Evimaria  and
      Crovella, Mark",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.656/",
    doi = "10.18653/v1/2025.findings-acl.656",
    pages = "12671--12688",
    ISBN = "979-8-89176-256-5",
    abstract = "Do LLMs process text and mathematics as a unified skill, or do these components rely on distinct underlying mechanisms? We investigate this question by disentangling the textual interpretation and mathematical solving steps in word problems drawn from Brazil{'}s largest college entrance exam (ENEM) and GSM8K, a popular grade school-level benchmark. Using the symbolic solver SymPy, we transform word problems into equivalent purely mathematical representations, isolating equation formulation from textual comprehension. Our extended benchmarks enable a structured analysis of LLM performance across these two dimensions. Through empirical evaluations, we find that small-scale LLMs struggle significantly more with text interpretation than with equation solving, with accuracy dropping by a factor of 2 to 7 when solving full word problems compared to their math-only counterparts. Exploratory factor analysis confirms a bidimensional structure in LLM reasoning, where models exhibit distinct proficiencies in textual and mathematical components, underscoring the need for targeted improvements in language comprehension. By analyzing the latent factors associated with each model, our findings provide a framework for researchers and practitioners to make informed choices when selecting models based on computational costs and the nature of their tasks."
}