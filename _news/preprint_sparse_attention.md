---
layout: post
title: We released a new pre-print "Sparse Attention Decomposition Applied to Circuit Tracing"
date: 2024-10-01 15:11:00-0400
inline: false
related_posts: false
---

The full text is available at [arXiv](https://arxiv.org/abs/2410.00340).

For a short explanation of the paper, check it out our Twitter post:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Can we identify the key signals moving between attention heads when a language model performs a task? Our paper (<a href="https://t.co/2fUtHa7BTF">https://t.co/2fUtHa7BTF</a>) offers new tools for this question. A key point of leverage is a new phenomenon we expose: sparse attention decomposition. Exploiting thisâ€¦</p>&mdash; Gabriel Franco (@gvsfranco) <a href="https://twitter.com/gvsfranco/status/1849210630668190135?ref_src=twsrc%5Etfw">October 23, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
